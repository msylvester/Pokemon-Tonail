* Generate Training Data

Run

#+begin_src 
python main.py --processes 0 --headless
#+end_src

to run headless, with processes = num cpus, and saving replay data (see env.replay_buffer) to the replays folder.

Note: training will run for the episode_length parameter, or it will stop early on encountering a battle. You can see logs of battles occuring.

** TODO "Swarming" ie good savepoints to explore from instead of starting tile


* Train (and run) a Q model

#+begin_src
python main.py --train_from_replays
#+end_src

Trains from the replay of all replays in the replay folder, in order.
It will then print out the filename containing the Q table of the model.
Finally, it will run the agent with the Q table and an exploration rate of 20%

** TODO Random sampling, repeated sampling, better sampling

* Running

Running train_from_replays will generate a pkl file for the resulting Q state. Load that file to have the AI agent play:

#+begin_src 
python main.py --initial_q_state <q_state_file>
#+end_src

** Manual mode

#+begin_src
python main.py --manual
#+end_src


* Visualizing results

=viz_rewards.py= will visualize the rewards on each tile integrated over all replays in the replays folder

=utils.py= can be used to quickly see summary stats, and can also visually debug individual episodes  with the --viz option

** TODO Integrate streamwrapper

Maybe do streamwrapper + heatmap?

